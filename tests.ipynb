{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytesseract\n",
    "from pdf2image import convert_from_path\n",
    "import PyPDF2\n",
    "import io\n",
    "import fitz # imports the pymupdf library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imagePDF_to_text function\n",
    "def imagePDF_to_text(file):\n",
    "    # poppler path\n",
    "    poppler_path=r\"Release-24.02.0-0/poppler-24.02.0/Library/bin\"\n",
    "    # convert to image\n",
    "    images = convert_from_path(file, poppler_path=poppler_path)\n",
    "    \n",
    "    pdf_writer = PyPDF2.PdfWriter()\n",
    "    page_text = ''\n",
    "    for image in images:\n",
    "        page = pytesseract.image_to_pdf_or_hocr(image, extension='pdf')\n",
    "        pdf = PyPDF2.PdfReader(io.BytesIO(page))\n",
    "\n",
    "        page_text += (pdf.pages[0]).extract_text()\n",
    "        pdf_writer.add_page(pdf.pages[0])\n",
    "    \n",
    "    return page_text\n",
    "\n",
    "# PDF_to_text function\n",
    "def PDF_to_text(file):\n",
    "  try:\n",
    "    # works only for text-based pdf\n",
    "    doc = fitz.open(file) # open a document\n",
    "    for page in doc: # iterate the document pages\n",
    "      text = page.get_text() # get plain text encoded as UTF-8\n",
    "      if not text.strip():\n",
    "        raise Exception('PDF is a scanned document')\n",
    "    print('Done!')\n",
    "    return text\n",
    "  except Exception as e:\n",
    "    print(f\"Error while processing PDF: {e}\")\n",
    "    print('Running OCR engine...')\n",
    "    text = imagePDF_to_text(file)\n",
    "    print('Done!\\n')\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_doc = r'files/Once upon a time.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done!\n"
     ]
    }
   ],
   "source": [
    "text = PDF_to_text(pdf_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90c47ea5e72b4826a8f4f09708f7bfe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/1.21k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spidy\\anaconda3\\envs\\dmml\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\spidy\\.cache\\huggingface\\hub\\models--google-t5--t5-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d54a4a5c23634d4cb84c3200091b8cda",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "spiece.model:   0%|          | 0.00/792k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e2d6a081113244fdb93ad7438fa98997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.39M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\spidy\\anaconda3\\envs\\dmml\\Lib\\site-packages\\transformers\\models\\t5\\tokenization_t5_fast.py:171: FutureWarning: This tokenizer was incorrectly instantiated with a model max length of 512 which will be corrected in Transformers v5.\n",
      "For now, this behavior is kept to avoid breaking backwards compatibility when padding/encoding with `truncation is True`.\n",
      "- Be aware that you SHOULD NOT rely on google-t5/t5-large automatically truncating your input to 512 when padding/encoding.\n",
      "- If you want to encode/pad to sequences longer than 512 you can either instantiate this tokenizer with `model_max_length` or pass `max_length` when encoding/padding.\n",
      "- To avoid this warning, please instantiate this tokenizer with `model_max_length` set to your preferred value.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4185409605554bf2a46e13fd5dd210ef",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/2.95G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b29084dcd9d44e8861f2f40c84cc1f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the forest spirit warned that not all who seek enlightenment are prepared for the trials that lie ahead. amidst the beauty and wonder of the forest, there lurked darker forces that sought to ensnare the unwary traveler. with cunning and courage, she outwitted the spider-witch and escaped her clutches.\n"
     ]
    }
   ],
   "source": [
    "# Load model directly\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google-t5/t5-large\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-large\")\n",
    "\n",
    "# Tokenize and generate summary\n",
    "inputs = tokenizer.encode(\"summarize: \" + text, return_tensors=\"pt\", max_length=1024, truncation=True)\n",
    "summary_ids = model.generate(inputs, max_length=150, min_length=40, length_penalty=2.0, num_beams=4, early_stopping=True)\n",
    "summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "# Forward pass through the model\n",
    "print(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dmml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
